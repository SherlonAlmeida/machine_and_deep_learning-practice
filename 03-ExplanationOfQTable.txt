Ao definir um caminho "gabarito" para o agente, a tabela vai sendo atualizada e convergindo
√† 1.0 nas posi√ß√µes que o agente passou. Ou seja, sempre que ele fizer um caminho que "Ganha"
a recompensa, ele refor√ßa o caminho passado na tabela Q.

Veja a seguir que o caminho definido manualmente para vencer o jogo faz a rota:
    Direita, Direita, Baixo, Baixo, Baixo, Direita
Ou seja:
    Quando o agente estiver no estado inicial (0,0) = 0, ele deve priorizar ir para a direita.
    Quando o agente estiver no estado (0,1) = 1, ele deve priorizar ir para a direita.
    Quando o agente estiver no estado (0,2) = 2, ele deve priorizar ir para baixo.
    Quando o agente estiver no estado (1,2) = 6, ele deve priorizar ir para baixo.
    Quando o agente estiver no estado (2,2) = 10, ele deve priorizar ir para baixo.
    Quando o agente estiver no estado (3,2) = 14, ele deve priorizar ir para direita.
    TODOS os demais estados n√£o foram mapeados na QTable a seguir pois foi definida apenas uma rota.
        Caso o sistema tivesse sido treinado automaticamente por refor√ßo, ele armazenaria as melhores
        a√ß√µes para cada estado atual.

#Actions: ‚óÄÔ∏è LEFT = 0, üîΩ DOWN = 1, ‚ñ∂Ô∏è RIGHT = 2, üîº UP = 3
actions_to_solve = [2,2,1,1,1,2] #Only for test purposes

Estados Esquerda    Baixo      Direita    Cima
0      [[0.         0.         0.71827642 0.        ]
1       [0.         0.         0.79453384 0.        ]
2       [0.         0.8519196  0.         0.        ]
3       [0.         0.         0.         0.        ]
4       [0.         0.         0.         0.        ]
5       [0.         0.         0.         0.        ]
6       [0.         0.90061337 0.         0.        ]
7       [0.         0.         0.         0.        ]
8       [0.         0.         0.         0.        ]
9       [0.         0.         0.         0.        ]
10      [0.         0.94975357 0.         0.        ]
11      [0.         0.         0.         0.        ]
12      [0.         0.         0.         0.        ]
13      [0.         0.         0.         0.        ]
14      [0.         0.         0.99998474 0.        ]
15      [0.         0.         0.         0.        ]]